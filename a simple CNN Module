import torch
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import matplotlib.pyplot as plt 
import torchvision                                   #数据库模块（图片数据）
import torch.utils.data as Data


torch.manual_seed(1)                                  #reproducible


#Hyper parameters 
EPOCH = 1                                            #training time
BATCH_SIZE = 50                                      #batch size
LR = 0.001                                           #learning rate
DOWNLOAD_MNIST = True                               #download or not ?


#Mnist 手写数字
train_data = torchvision.datasets.MNIST(              #training data
    root = "./mnist/", 
    train = True,
    transform = torchvision.transforms.ToTensor(),      #from(0,255)->(0,1)
    download = DOWNLOAD_MNIST
)



#plot some examples(photos and labels)
print(train_data.train_data.size())                   #output dataset's size
print(train_data.train_labels.size())                 #output dataset's labels
for i in range(3):
    plt.figure()
    plt.imshow(train_data.train_data[i].numpy(), cmap = "gray")  #plot photos
    plt.title("%i" % train_data.train_labels[i])                 #plot labels
    plt.show()

    
    
loader = Data.DataLoader(                             #loader
    dataset = train_data,                             #dataset
    batch_size = BATCH_SIZE,                          #batch size
    shuffle = True,                                  #shuffle or not ?
    num_workers = 2,                                 #muti-threading
)



test_data = torchvision.datasets.MNIST(              #testing data
    root = "./mnist", 
    train = False,
)


                                                                                                                        #test x,y
test_x = Variable(torch.unsqueeze(test_data.test_data, dim = 1), volatile = True).type(torch.FloatTensor)[:2000]/255
test_y = test_data.test_labels[:2000]


class CNN(torch.nn.Module):                     #CNN(convolution neural network)继承于torch.nn.Module
        def __init__(self):                     #初始化
            super(CNN, self).__init__()         #调用父类函数
            self.conv1 = nn.Sequential(         #第一个卷积网络                        input shape(1, 28, 28)
                nn.Conv2d(                      #卷积层                     
                    in_channels = 1,            #input height
                    out_channels = 16,          #n_filer                                     shape(16, 28, 28)
                    kernel_size = 5,            #filer_size:5*5
                    stride = 1,                 #filer_step:1
                    padding = 2,                # if stride = 1, padding = (kernel_size - 1)/2 = (5-1)/2，  padding:填充
                ), 
                nn.ReLU(),                      #激活函数ReLU                                shape(16, 28, 28)
                nn.MaxPool2d(kernel_size = 2)   #池化层:Maxpooling:filer大小2*2       output shape(16, 14, 14)
            )
            self.conv2 = nn.Sequential(        #第二个卷积网络
                nn.Conv2d(16, 32, 5, 1, 2),    #卷积层及其参数                          input shape(16, 14, 14)
            nn.ReLU(),                         #                                              shape(32, 14, 14)
            nn.MaxPool2d(2)                    #                                              shape(32, 7, 7)
            )
            self.out = nn.Linear(32 * 7 * 7, 10)
        def forward(self, x):
            x = self.conv1(x)
            x = self.conv2(x)                 #(batch, 32, 7, 7)
            x = x.view(x.size(0), -1)         #(batch, 32 * 7 * 7)
            output = self.out(x)
            return output

        
cnn = CNN()
print(cnn)



optimizer = torch.optim.Adam(cnn.parameters(), lr = LR)      #optimizer
loss_function = nn.CrossEntropyLoss()                        #loss_function


#minibatch training 
for epoch in range(EPOCH):
    for step, (x, y) in enumerate(loader):
                                          #input
        b_x = Variable(x)                 #batch x
        b_y = Variable(y)                 #batch y
        
        
        output = cnn(b_x)                   #output
        loss = loss_function(output, b_y)   #loss
        
        
        optimizer.zero_grad()              #残留的参数置0
        loss.backward()                    #误差反向传播
        optimizer.step()                   #更新参数
        
        
#training proccess
        if step%50 == 0:
           test_output = cnn(test_x)                            #output
           pred_y = torch.max(test_output, 1)[1].data.squeeze() #prediction
           accuracy = sum(pred_y == test_y)/test_y.size(0)      #accuracy
           print("Epoch:", epoch, " |step:", step, " |train's loss: %.4f"% loss.data[0], "accuracy:", accuracy) 
print("training over!")



print("testing......")
print("test result:")
test_output = cnn(test_x[:100])
pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()
print(pred_y, "prediction number")
print(test_y[:100].numpy(), "real number")
